# Configuration used to run the pipeline
project_id: gid-ml-framework
region: europe-west4
run_config:
  # Name of the image to run as the pipeline steps
  image: europe-west4-docker.pkg.dev/gid-ml-framework/gid-ml-framework-data/recommender_gnn:latest

  # Pull policy to be used for the steps. Use Always if you push the images
  # on the same tag, or Never if you use only local images
  image_pull_policy: Always

  # Location of Vertex AI GCS root
  root: gid-ml-framework-data/temp

  # Prefix of Vertex AI pipeline run
  experiment_name: gid-ml-framework-gnns-santander

  # Name of the scheduled run, templated with the schedule parameters
  scheduled_run_name: gid-ml-framework-gnns-santander

  run_name: gid-ml-framework-gnns-santander-${run_id}

  # Optional service account to run vertex AI Pipeline with
  # service_account: iap-accessor@gid-advanced-analytics-infra.iam.gserviceaccount.com
  # for the nodes. When not provided they're set to 500m cpu and 1024Mi memory.
  # If you don't want to specify pipeline resources set both to None in __default__.

  # Optional pipeline description
  # description: "Very Important Pipeline"

  # Optional section allowing adjustment of the resources, reservations and limits
  # for the nodes. When not provided they're set to 500m cpu and 1024Mi memory.
  # If you don't want to specify pipeline resources set both to None in __default__.
  resources:
    # Default settings for the nodes
    __default__:
      cpu: 1
      memory: 16Gi

    test-gpu-node:
      gpu: 1
      cpu: 7.91
      memory: 26Gi

    test-tag:
      gpu: 1

    # # Training nodes can utilize more than one CPU if the algorithm
    # # supports it
    santander-dgsr-kaggle-gr-train-model-node:
      gpu: 1
      cpu: 7.91
      memory: 26Gi

    graph-recommendation-modeling-preprocess-node:
      cpu: 8
      memory: 32Gi
    
  node_selectors:
    test-tag-2:
      gke_accelerator: NVIDIA_TESLA_T4



  # Optional section allowing to generate config files at runtime,
  # useful e.g. when you need to obtain credentials dynamically and store them in credentials.yaml
  # but the credentials need to be refreshed per-node
  # (which in case of Vertex AI would be a separate container / machine)
  # Example:
  # dynamic_config_providers:
  #   - cls: kedro_vertexai.auth.gcp.MLFlowGoogleIAMCredentialsProvider
  #     params:
  #       client_id: 260364966542-721p48q045h4o2hmsmg5dl0gksm7udiu.apps.googleusercontent.com
  #       service_account: iap-accessor@gid-advanced-analytics-infra.iam.gserviceaccount.com
  dynamic_config_providers: []

  # MLflow IAM authorization
  mlflow:
    request_header_provider_params:
      client_id: 260364966542-721p48q045h4o2hmsmg5dl0gksm7udiu.apps.googleusercontent.com
      service_account: iap-accessor@gid-advanced-analytics-infra.iam.gserviceaccount.com
